{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638a169e",
   "metadata": {},
   "source": [
    "# Wasserstein GAN - Gradient Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7ada6",
   "metadata": {},
   "source": [
    "- GAN의 손실 함수를 개선해 기울기 소실이나 모드 붕괴 현상을 완화하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a944333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123f9d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU\n"
     ]
    }
   ],
   "source": [
    "epochs, batch_size = 100, 64\n",
    "lr,b1,b2 = 2e-4,0.5,0.999\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "channels = 1\n",
    "n_critic = 5\n",
    "lambda_gp = 10\n",
    "img_shape = (channels, img_size, img_size)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Train on GPU\")\n",
    "    cuda = True\n",
    "else:\n",
    "    print(\"Train on CPU\")\n",
    "    cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b02b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"FashionMNIST_DATASET\",exist_ok=True)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        'FashionMNIST_DATASET',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5],[0.5])\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed6929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)),512),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(512,256),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        flat_img = img.view(img.shape[0],-1)\n",
    "        pred = self.model(flat_img)\n",
    "        return pred\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "    \n",
    "        def block(in_feature, out_feature, normalize=True):\n",
    "            layers = [nn.Linear(in_feature,out_feature)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feature,0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2,inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim,128,normalize=False),\n",
    "            *block(128,256),\n",
    "            *block(256,512),\n",
    "            *block(512,1024),\n",
    "            nn.Linear(1024,int(np.prod(img_shape))),\n",
    "            nn.Tanh() # -1 ~ 1 (CLIPPING)\n",
    "        )\n",
    "    def forward(self,z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0],*img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a221cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()\n",
    "if cuda:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def gradient_penalty(D,real_img,fake_img):\n",
    "    alpha = Tensor(np.random.random((real_img.size(0),1,1,1)))\n",
    "    interpolates = (alpha * real_img + ((1-alpha)*fake_img)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_img.shape[0],1).fill_(1.0),requires_grad=False)\n",
    "    gradients = autograd.grad(outputs = d_interpolates,\n",
    "                              inputs=interpolates,\n",
    "                              grad_outputs=fake,\n",
    "                              create_graph=True,\n",
    "                              retain_graph=True,\n",
    "                              only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0),-1)\n",
    "    GP = ((gradients.norm(2,dim=1)-1) **2).mean()\n",
    "    return GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca145be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyssk\\AppData\\Local\\Temp\\ipykernel_2172\\1925376071.py:12: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  z = Variable(Tensor(np.random.normal(0,1,(imgs.shape[0],latent_dim))))\n",
      "c:\\Users\\hyssk\\anaconda3\\envs\\pytorch_cuda_11_8\\lib\\site-packages\\torch\\autograd\\graph.py:768: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1/100 | [D loss: -3.7432658672332764] | [G loss: 0.43370795249938965]\n",
      "EPOCH : 2/100 | [D loss: -2.8184943199157715] | [G loss: -1.4357749223709106]\n",
      "EPOCH : 3/100 | [D loss: -3.0436651706695557] | [G loss: -1.6627751588821411]\n",
      "EPOCH : 4/100 | [D loss: -2.319378137588501] | [G loss: -0.7082768678665161]\n",
      "EPOCH : 5/100 | [D loss: -2.835312843322754] | [G loss: -0.7042218446731567]\n",
      "EPOCH : 6/100 | [D loss: -1.454805850982666] | [G loss: -0.16897116601467133]\n",
      "EPOCH : 7/100 | [D loss: -2.4491446018218994] | [G loss: -2.0427308082580566]\n",
      "EPOCH : 8/100 | [D loss: -2.2560102939605713] | [G loss: -0.3144831657409668]\n",
      "EPOCH : 9/100 | [D loss: -2.2736639976501465] | [G loss: 0.6223976016044617]\n",
      "EPOCH : 10/100 | [D loss: -2.8120322227478027] | [G loss: -1.374320387840271]\n",
      "EPOCH : 11/100 | [D loss: -2.0508322715759277] | [G loss: -2.15464186668396]\n",
      "EPOCH : 12/100 | [D loss: -2.7060093879699707] | [G loss: -0.5170355439186096]\n",
      "EPOCH : 13/100 | [D loss: -1.3908313512802124] | [G loss: -1.7763633728027344]\n",
      "EPOCH : 14/100 | [D loss: -2.612666368484497] | [G loss: -2.185049533843994]\n",
      "EPOCH : 15/100 | [D loss: -1.8055331707000732] | [G loss: 0.9707103371620178]\n",
      "EPOCH : 16/100 | [D loss: -2.2387001514434814] | [G loss: -1.0006941556930542]\n",
      "EPOCH : 17/100 | [D loss: -1.9237498044967651] | [G loss: -0.3090989887714386]\n",
      "EPOCH : 18/100 | [D loss: -1.9332839250564575] | [G loss: -1.425866723060608]\n",
      "EPOCH : 19/100 | [D loss: -1.5916019678115845] | [G loss: 0.6124569177627563]\n",
      "EPOCH : 20/100 | [D loss: -1.5789936780929565] | [G loss: -0.8709965944290161]\n",
      "EPOCH : 21/100 | [D loss: -1.47673499584198] | [G loss: -2.458972215652466]\n",
      "EPOCH : 22/100 | [D loss: -1.1050806045532227] | [G loss: -2.336026668548584]\n",
      "EPOCH : 23/100 | [D loss: -1.3483984470367432] | [G loss: -1.587787389755249]\n",
      "EPOCH : 24/100 | [D loss: -1.3531949520111084] | [G loss: -0.08768486976623535]\n",
      "EPOCH : 25/100 | [D loss: -1.9755152463912964] | [G loss: -0.8102014064788818]\n",
      "EPOCH : 26/100 | [D loss: -1.8739347457885742] | [G loss: -0.2876889407634735]\n",
      "EPOCH : 27/100 | [D loss: -0.9841140508651733] | [G loss: -1.2794251441955566]\n",
      "EPOCH : 28/100 | [D loss: -1.927490234375] | [G loss: 0.7735152840614319]\n",
      "EPOCH : 29/100 | [D loss: -1.574227213859558] | [G loss: -0.08177895843982697]\n",
      "EPOCH : 30/100 | [D loss: -1.6679518222808838] | [G loss: -0.36809226870536804]\n",
      "EPOCH : 31/100 | [D loss: -1.3644592761993408] | [G loss: -0.7176721096038818]\n",
      "EPOCH : 32/100 | [D loss: -0.8813859224319458] | [G loss: -1.7223209142684937]\n",
      "EPOCH : 33/100 | [D loss: -1.9975314140319824] | [G loss: -0.040750764310359955]\n",
      "EPOCH : 34/100 | [D loss: -1.9822286367416382] | [G loss: -1.6793453693389893]\n",
      "EPOCH : 35/100 | [D loss: -1.4688676595687866] | [G loss: -1.4486221075057983]\n",
      "EPOCH : 36/100 | [D loss: -1.4416428804397583] | [G loss: -2.1438305377960205]\n",
      "EPOCH : 37/100 | [D loss: -2.337801456451416] | [G loss: -1.0141475200653076]\n",
      "EPOCH : 38/100 | [D loss: -1.913478136062622] | [G loss: -1.0019497871398926]\n",
      "EPOCH : 39/100 | [D loss: -1.537419319152832] | [G loss: -0.6709154844284058]\n",
      "EPOCH : 40/100 | [D loss: -1.1756131649017334] | [G loss: -2.4639077186584473]\n",
      "EPOCH : 41/100 | [D loss: -2.0200695991516113] | [G loss: -1.8830705881118774]\n",
      "EPOCH : 42/100 | [D loss: -1.9721391201019287] | [G loss: -0.9613054990768433]\n",
      "EPOCH : 43/100 | [D loss: -1.4868422746658325] | [G loss: -1.1028332710266113]\n",
      "EPOCH : 44/100 | [D loss: -2.63189435005188] | [G loss: -2.3033337593078613]\n",
      "EPOCH : 45/100 | [D loss: -1.127725601196289] | [G loss: -0.6993354558944702]\n",
      "EPOCH : 46/100 | [D loss: -1.1846047639846802] | [G loss: -0.8824979066848755]\n",
      "EPOCH : 47/100 | [D loss: -1.8863849639892578] | [G loss: 0.2881075441837311]\n",
      "EPOCH : 48/100 | [D loss: -1.1954725980758667] | [G loss: -0.8448282480239868]\n",
      "EPOCH : 49/100 | [D loss: -1.7001463174819946] | [G loss: -1.3268147706985474]\n",
      "EPOCH : 50/100 | [D loss: -1.0568842887878418] | [G loss: -0.6411837339401245]\n",
      "EPOCH : 51/100 | [D loss: -2.098767042160034] | [G loss: -1.223783016204834]\n",
      "EPOCH : 52/100 | [D loss: -1.7846933603286743] | [G loss: 0.2568180561065674]\n",
      "EPOCH : 53/100 | [D loss: -0.5461628437042236] | [G loss: 0.33875027298927307]\n",
      "EPOCH : 54/100 | [D loss: -0.8263835906982422] | [G loss: -1.4830069541931152]\n",
      "EPOCH : 55/100 | [D loss: -1.1623491048812866] | [G loss: -1.3251490592956543]\n",
      "EPOCH : 56/100 | [D loss: -1.9189256429672241] | [G loss: -1.7560882568359375]\n",
      "EPOCH : 57/100 | [D loss: -1.1480659246444702] | [G loss: -1.3665977716445923]\n",
      "EPOCH : 58/100 | [D loss: -2.1776387691497803] | [G loss: 0.2064938247203827]\n",
      "EPOCH : 59/100 | [D loss: -1.1465833187103271] | [G loss: 0.4847952723503113]\n",
      "EPOCH : 60/100 | [D loss: -1.1948606967926025] | [G loss: -0.5213390588760376]\n",
      "EPOCH : 61/100 | [D loss: -1.40214204788208] | [G loss: -0.6392580270767212]\n",
      "EPOCH : 62/100 | [D loss: -1.081252098083496] | [G loss: -1.3441487550735474]\n",
      "EPOCH : 63/100 | [D loss: -0.9030113220214844] | [G loss: -1.1115741729736328]\n",
      "EPOCH : 64/100 | [D loss: -0.9480354189872742] | [G loss: -0.9581274390220642]\n",
      "EPOCH : 65/100 | [D loss: -2.0656981468200684] | [G loss: -0.16266505420207977]\n",
      "EPOCH : 66/100 | [D loss: -1.5229008197784424] | [G loss: 0.1766185164451599]\n",
      "EPOCH : 67/100 | [D loss: -1.518667459487915] | [G loss: -1.2876148223876953]\n",
      "EPOCH : 68/100 | [D loss: -1.3091868162155151] | [G loss: -0.8054392337799072]\n",
      "EPOCH : 69/100 | [D loss: -0.6449815034866333] | [G loss: -0.6762411594390869]\n",
      "EPOCH : 70/100 | [D loss: -1.2389811277389526] | [G loss: -1.6416847705841064]\n",
      "EPOCH : 71/100 | [D loss: -0.46231967210769653] | [G loss: 0.3498072326183319]\n",
      "EPOCH : 72/100 | [D loss: -1.1769812107086182] | [G loss: -1.127593994140625]\n",
      "EPOCH : 73/100 | [D loss: -1.281374454498291] | [G loss: -2.123870372772217]\n",
      "EPOCH : 74/100 | [D loss: -0.8319766521453857] | [G loss: -2.1070642471313477]\n",
      "EPOCH : 75/100 | [D loss: -1.1549092531204224] | [G loss: -1.6980407238006592]\n",
      "EPOCH : 76/100 | [D loss: -1.6407021284103394] | [G loss: -0.6366846561431885]\n",
      "EPOCH : 77/100 | [D loss: -1.1487810611724854] | [G loss: -0.19973525404930115]\n",
      "EPOCH : 78/100 | [D loss: -1.3206819295883179] | [G loss: -0.17009153962135315]\n",
      "EPOCH : 79/100 | [D loss: -1.5942964553833008] | [G loss: -0.598494291305542]\n",
      "EPOCH : 80/100 | [D loss: -1.6491518020629883] | [G loss: -0.08251933753490448]\n",
      "EPOCH : 81/100 | [D loss: -0.7655197381973267] | [G loss: 0.4101451337337494]\n",
      "EPOCH : 82/100 | [D loss: -1.634745717048645] | [G loss: -0.8791571259498596]\n",
      "EPOCH : 83/100 | [D loss: -0.9787926077842712] | [G loss: -1.8444398641586304]\n",
      "EPOCH : 84/100 | [D loss: -1.7662357091903687] | [G loss: -1.6020630598068237]\n",
      "EPOCH : 85/100 | [D loss: -1.072348713874817] | [G loss: -0.16871272027492523]\n",
      "EPOCH : 86/100 | [D loss: -0.9488758444786072] | [G loss: -1.847553014755249]\n",
      "EPOCH : 87/100 | [D loss: -1.1734585762023926] | [G loss: -0.4648919105529785]\n",
      "EPOCH : 88/100 | [D loss: -0.9294872283935547] | [G loss: -0.34729698300361633]\n",
      "EPOCH : 89/100 | [D loss: -0.6315792798995972] | [G loss: -1.235872507095337]\n",
      "EPOCH : 90/100 | [D loss: -1.736202359199524] | [G loss: -0.5296392440795898]\n",
      "EPOCH : 91/100 | [D loss: -1.0640649795532227] | [G loss: 0.14754757285118103]\n",
      "EPOCH : 92/100 | [D loss: -0.8825448751449585] | [G loss: -1.798051118850708]\n",
      "EPOCH : 93/100 | [D loss: -1.061954140663147] | [G loss: -1.183652400970459]\n",
      "EPOCH : 94/100 | [D loss: -1.207951307296753] | [G loss: -1.418923258781433]\n",
      "EPOCH : 95/100 | [D loss: -0.766394317150116] | [G loss: -1.5660970211029053]\n",
      "EPOCH : 96/100 | [D loss: -0.8359794020652771] | [G loss: -1.087459921836853]\n",
      "EPOCH : 97/100 | [D loss: -1.073628306388855] | [G loss: -1.018744945526123]\n",
      "EPOCH : 98/100 | [D loss: -1.3932244777679443] | [G loss: -1.297914743423462]\n",
      "EPOCH : 99/100 | [D loss: -1.6674124002456665] | [G loss: -1.3611645698547363]\n",
      "EPOCH : 100/100 | [D loss: -1.1601855754852295] | [G loss: -1.402801752090454]\n"
     ]
    }
   ],
   "source": [
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = lr, betas=(b1,b2))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(),lr=lr,betas=(b1,b2))\n",
    "current_iters = 0\n",
    "os.makedirs(\"WGAN-GP_results\",exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        ##Train Discriminator ##\n",
    "        optimizer_D.zero_grad()\n",
    "        z = Variable(Tensor(np.random.normal(0,1,(imgs.shape[0],latent_dim))))\n",
    "        \n",
    "        fake_imgs = G(z)\n",
    "        real_pred = D(real_imgs)\n",
    "        fake_pred = D(fake_imgs)\n",
    "        GP = gradient_penalty(D, real_imgs.data, fake_imgs.data)\n",
    "        d_loss = -torch.mean(real_pred) + torch.mean(fake_pred) + lambda_gp*GP\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        if i % n_critic == 0:\n",
    "            ## Train G ##\n",
    "            fake_imgs = G(z)\n",
    "            fake_pred = D(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_pred)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            current_iters += n_critic\n",
    "            \n",
    "    print(f'EPOCH : {epoch + 1}/{epochs} | [D loss: {d_loss.item()}] | [G loss: {g_loss.item()}]')\n",
    "    save_image(fake_imgs.data[:25],f\"WGAN-GP_results/epochs_{epoch}.png\",nrow=5,normalize=True)\n",
    "    \n",
    "torch.save(G.state_dict(),'./Generator.pth')\n",
    "torch.save(D.state_dict(),'./Discriminator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda_11_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
